{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 10 - Processamento de Linguagem Natural</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** Este Jupyter Notebook foi atualizado para a versão 3.6.1. da Linguagem Python em 05/07/2017 ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking and Parsing\n",
    "\n",
    "**Chunking ou shallow parsing** é uma análise de uma sentença que identifica os constituintes (grupos de substantivos, verbos, grupos de verbos, etc.), mas não especifica sua estrutura interna, nem seu papel na sentença principal.\n",
    "\n",
    "![chunking](http://www.nltk.org/book/tree_images/ch07-tree-1.png)\n",
    "\n",
    "**Parsing ou syntactic analysis** é o processo de análise de uma sequência de símbolos, seja em linguagem natural ou em linguagens computacionais, conforme as regras de uma gramática formal.\n",
    "\n",
    "![parsing](http://www.nltk.org/book/tree_images/ch08-tree-4.png)\n",
    "\n",
    "---\n",
    "Tanto a Chunking como Parsing podem ser resolvidas com dois métodos:\n",
    "\n",
    "* **Grammars**\n",
    "* **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking usa uma sintaxe de regexp especial para regras que delimitam os pedaços. Essas regras devem ser convertidas em expressões regulares antes que uma frase possa ser fragmentada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/book/ch07.html#code-unigram-chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_text = \"[ The/DT cat/NN ] sat/VBD on/IN [ the/DT mat/NN ] [ the/DT dog/NN ] chewed/VBD ./.\"\n",
    "gold_chunked_text = tagstr2tree(tagged_text)\n",
    "unchunked_text = gold_chunked_text.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(<(DT)>)?(<(JJ)>)*(<(NN[^\\\\{\\\\}<>]*)>)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_pattern = \"<DT>?<JJ>*<NN.*>\"\n",
    "regexp_pattern = tag_pattern2re_pattern(tag_pattern)\n",
    "regexp_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Parser: http://nlp.stanford.edu/software/lex-parser.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint Grammar : http://beta.visl.sdu.dk/constraint_grammar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Floresta é um corpus para Português do Brasil e de Portugal, disponível no NLTK\n",
    "from nltk.corpus import floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = floresta.parsed_sents()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(STA+fcl\n",
      "  (SUBJ+np (>N+art O) (H+prop 7_e_Meio))\n",
      "  (P+v-fin é)\n",
      "  (SC+np\n",
      "    (>N+art um)\n",
      "    (H+n ex-libris)\n",
      "    (N<+pp\n",
      "      (H+prp de)\n",
      "      (P<+np (>N+art a) (H+n noite) (N<+adj algarvia))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[STA+fcl -> SUBJ+np P+v-fin SC+np .,\n",
       " SUBJ+np -> >N+art H+prop,\n",
       " >N+art -> 'O',\n",
       " H+prop -> '7_e_Meio',\n",
       " P+v-fin -> 'é',\n",
       " SC+np -> >N+art H+n N<+pp,\n",
       " >N+art -> 'um',\n",
       " H+n -> 'ex-libris',\n",
       " N<+pp -> H+prp P<+np,\n",
       " H+prp -> 'de',\n",
       " P<+np -> >N+art H+n N<+adj,\n",
       " >N+art -> 'a',\n",
       " H+n -> 'noite',\n",
       " N<+adj -> 'algarvia',\n",
       " . -> '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=http://facebook.com/dsacademy>facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
